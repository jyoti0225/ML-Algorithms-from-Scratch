# ML-Algorithms-from-Scratch
In this repository, I will be implementing popular machine learning algorithms right from scratch.

## [1. Decision Tree Algorithm](https://github.com/jyoti0225/ML-Algorithms-from-Scratch/tree/master/Decision%20Tree)</br>
For most of complex and non-linear data , tree based algorithms like Decision Tree, Random Forest, XGBoost, etc works better than most of the algorithms. But have you ever thought how these algorithms work?<br>
In this, you will understand the working of a Decision Tree and then can also implement it in python using NumPy. Here is an example:

<img src="https://annalyzin.files.wordpress.com/2016/07/decision-trees-example-tutorial.png" height= "50%" width="50%"></img>

## [2. K-Nearesr Neighbour Algorithm](https://github.com/jyoti0225/ML-Algorithms-from-Scratch/tree/master/K%20Nearest%20Neighbor)</br>
In this, we will be implementing KNN-Algorithm from scratch. KNN is a data classification algorithm that attempts to determine what group a data point is in by looking at the data points around it.

<br>
<div style="float:centre">
<img src= "https://github.com/jyoti0225/ML-Algorithms-from-Scratch/blob/master/K%20Nearest%20Neighbor/knn_image.png"  height= "30%" width="30%">
</di>

## [3. K-Means Algorithm](https://github.com/jyoti0225/ML-Algorithms-from-Scratch/tree/master/K-Means)</br>
In this, we will be implementing K-Means Algorithm from scratch and shows an example application - reducing the number of colors.
K-means is an entry level unsupervised clustering algorithm. In contrast to traditional methods in unsupervised methods we have data points, but we don't have labels. Based on distances between data algorithm finds k groups of points in a given dataset.
<div style="float:centre">
<img src= "https://github.com/jyoti0225/ML-Algorithms-from-Scratch/blob/master/K-Means/k-means.gif"  height= "60%" width="60%" >
</di>
